CHATGPT:

Отлично, я подготовлю предложения по улучшению визуального качества маски сегментации в текущем пайплайне Unity AR Foundation + Sentis, включая сглаживание, апскейлинг и возможные GPU-оптимизации, не затрагивая AR-геометрию или замену модели. Как только будут готовы наработки — сообщу.


# Улучшение визуального качества сегментации стен в Unity AR-приложении

## Проблема блоков и размытых границ сегментации

Низкое разрешение выходной маски (например, 120×160) приводит к «блочности» – видимым квадратикам и зазубренным краям при масштабировании до размера экрана. Цель – сгладить эти артефакты и получить чёткие границы сегментации стены, **не изменяя модель** (SegFormer) и сохраняя текущую архитектуру (Unity AR Foundation + Sentis). Ниже рассмотрены различные подходы улучшения качества маски стен:

## Масштабирование маски с помощью фильтрации (билинейное и др.)

**Билинейная интерполяция** – базовый способ сгладить блоки при увеличении изображения. Unity поддерживает билинейный фильтр текстур: при увеличении текстуры пиксели интерполируются между соседними, что смягчает резкие ступени. В вашем случае, если модель выдаёт маску 120×160, а для отображения используется `RenderTexture` 640×480, убедитесь, что включён билинейный фильтр (например, `segmentationMaskTexture.filterMode = FilterMode.Bilinear`). Согласно документации Sentis, при несоответствии размеров тензора и целевого RenderTexture автоматически применяется **линейная ресемплинг-интерполяция** (т.е. билинейная). Это означает, что вызов `Sentis.TextureConverter.RenderToTexture(tensor, renderTexture)` сам по себе должен выполнять билинейное масштабирование маски (при условии, что используемая версия Sentis ≥1.3).

**Практика масштабирования**: Можно выводить тензор напрямую в RenderTexture меньшего размера (например, 160×120), а затем отдельно масштабировать его до 640×480 с билинейной фильтрацией. Один из способов – использовать `Graphics.Blit` или отрисовать мелкую текстуру на экран с помощью вторичной камеры/Canvas. Как отмечалось экспертами, можно устроить двухступенчатый рендер: **сначала** отрисовать вывод модели в низком разрешении, **затем** отрендерить эту текстуру на весь экран с кастомным шейдером, выполняющим апскейлинг и анти-алиасинг. Такой подход даёт контроль над алгоритмом апскейлинга – можно реализовать не только билинейный, но и более продвинутый, например бикубический или Lanczos фильтр, которые могут дать более гладкие результаты (хотя и с большим числом выборок текселей).

**Пример билинейного масштабирования через Blit** (C#):

```csharp
// Предполагается outputTensor – тензор [1,1,H,W] или [1,C,H,W] с маской
RenderTexture lowResMask = RenderTexture.GetTemporary(160, 120, 0, RenderTextureFormat.R8);
RenderTexture highResMask = new RenderTexture(640, 480, 0, RenderTextureFormat.R8);
lowResMask.enableRandomWrite = false;
lowResMask.filterMode = FilterMode.Point;    // точечный при заполнении из тензора
highResMask.filterMode = FilterMode.Bilinear; // билинейный при выборке
// Шаг 1: вывести тензор в lowResMask (1:1 без сглаживания)
Sentis.TextureConverter.RenderToTexture(outputTensor, lowResMask);
// Шаг 2: билинейно растянуть lowResMask в highResMask
Graphics.Blit(lowResMask, highResMask);
```

После этого `highResMask` можно отображать на UI. **Важно:** при прямом выводе тензора сразу в `RenderTexture` большего размера Sentis сам выполнит линейное масштабирование, что упрощает задачу. Однако явный двухэтапный подход позволяет дополнительно обработать маску между шагами (например, применить шейдер).

**Альтернативные фильтры**: Билинейный фильтр сглаживает блоки, но может слегка размывать границы. Если нужны более чёткие, но *не зубчатые* границы, можно реализовать кастомный **бикубический фильтр** или специализированный апскейлинг. Такие фильтры учитывают больше соседних пикселей при интерполяции, давая более резкий результат. Unity не имеет готового бикубического масштабирования по умолчанию, но вы можете написать шейдер, выборочно самплирующий 16 ближайших текселей и вычисляющий значение по бикубической формуле. Это увеличит нагрузку (больше выборок), но позволит достичь лучшего качества апскейла по сравнению с билинейным.

## Сглаживание маски с постобработкой (morphological, blur, threshold)

**Морфологические операции** – эффективный способ устранения пикселизации на бинарных масках. Техника *dilation + erosion* (расширение + сужение, также известна как **морфологическое замыкание** “closing”) сглаживает “рваные” края маски. Расширение (dilation) заполняет мелкие промежутки и выпирающие пиксели, немного утолщая маску, а последующее сужение (erosion) возвращает границы ближе к исходному положению, но уже без мелких зазубрин. В результате контуры становятся более плавными и округлыми. Например, разработчики моделей сегментации рекомендуют применять **расширение/сужение или гауссовое размытие** для получения более гладких масок. Эти операции можно выполнить на GPU с помощью шейдера или ComputeShader, пробегая по окрестности каждого пикселя:

* *Dilation:* для каждого пикселя взять максимальное значение в окрестности (например, 3×3) исходной маски.
* *Erosion:* для каждого пикселя взять минимальное значение в окрестности.

В псевдошейдере расширение+сужение на 3×3 можно описать так:

```glsl
// uniform sampler2D _MaskTex;
float3 val = tex2D(_MaskTex, i.uv).rgb; // текущее значение (для цветной маски – rgb)
float maxNei = 0.0;
for(int ox=-1; ox<=1; ++ox) {
    for(int oy=-1; oy<=1; ++oy) {
       float nei = tex2D(_MaskTex, i.uv + float2(ox, oy) * _TexelSize).r;
       maxNei = max(maxNei, nei);
    }
}
float dilated = maxNei;        // расширенная маска
// ... затем аналогично пройти по dilated-изображению для erosion ...
```

Для бинарной маски (0/1) maxNei фактически заполнит единицами все дырочки до 1-пиксельного размера. После последующего erosion (min по окрестности) остаются более гладкие границы. Эти операции эффективно выполняются на GPU, занимая небольшое время на маске 640×480.

**Гауссово размытие + порог** – другой подход к сглаживанию. Вместо дискретных морфологических операций, мы мягко размываем границы и затем снова применяем жёсткий порог, чтобы вернуть бинарность. Например, как советует автор Ultralytics, можно применить Gaussian blur (например, ядро 5×5) к маске, а потом превратить её обратно в чёрно-белую через threshold 0.5. Размытие убирает резкие ступеньки, а порог **срезает полутоновый переход**, оставляя более ровную линию. В Unity это можно реализовать шейдером в два прохода: первый делает размытие (можно двумя проходами 5×5 сепарабельно или напрямую), второй – пороговую фильтрацию. Ниже концептуальный пример:

```glsl
// Франгмент шейдера: Blur (одно из направлений, для простоты не разделяем по горизонт/вертикаль)
float sum = 0.0;
const float kernel[5] = float[](0.061, 0.244, 0.389, 0.244, 0.061); // веса Gaussian 5x5 (1D)
// Проход по 5 текселей по X (для горизонтального размытия)
for(int i=-2; i<=2; ++i) {
    sum += tex2D(_MaskTex, i.uv + float2(i,0)*_TexelSize).r * kernel[i+2];
}
return float4(sum, sum, sum, 1.0);
// Второй проход (вертикальный) делается аналогично или сразу 2D свёрткой
// После получения размытой маски применяем threshold:
float blurredVal = tex2D(_BlurredMask, i.uv).r;
float binaryVal = blurredVal > 0.5 ? 1.0 : 0.0;
```

На практике можно объединить оба шага в один compute shader или пиксельный шейдер с большим ядром, но двухпроходный гаусс (сепарабельный) обычно быстрее.

**Результат:** и морфологическое замыкание, и blur+threshold приводят к **чистым бинарным границам без мелкой пикселизации**. Разница в том, что после морфологической фильтрации контур может слегка сместиться или сгладиться углы; после blur+threshold контур сохраняет положение крупных деталей, но избавляется от ступенек, приобретая слегка более сглаженную форму. Рекомендуется поэкспериментировать с размерами ядра/окна: 3×3 уберёт самые мелкие ступени, 5×5 сильнее сгладит (но может чрезмерно упростить форму). Unity Sentis не предоставляет готовых функций морфологии, но эти фильтры легко добавить самостоятельно на GPU.

## Замена CPU-постобработки на GPU (шейдеры вместо CPU-фильтров)

Вы упомянули, что уже применяете фильтры резкости и контраста на CPU, что добавляет \~1.5 мс. Этот шаг можно перенести на GPU, сведя к минимуму задержки и избавившись от дорогостоящего чтения текстуры на CPU. **Материал с шейдером** можно назначить на `RawImage` или выполнить через `Graphics.Blit`, чтобы реализовать те же эффекты:

* **Повышение контраста маски**: Если маска представлена как оттенки серого (например, после билинейного апскейлинга), контраст можно увеличить, сдвигая значения ближе к 0 или 1. В шейдере это делается нелинейной операцией. Например: `color = saturate((color - 0.5) * contrastFactor + 0.5)`, где `contrastFactor > 1` усиливает контраст (значения около 0.5 станут еще ближе к 0 или 1). Для бинарной маски, по сути, контрастное усиление плюс мягкое размытие эквивалентно применению порога.

* **Усиление резкости (sharpen)**: Шейдер может выполнять свёртку ядром резкости. Простейшее ядро для повышения резкости:
  $\begin{pmatrix} 0 & -1 & 0 \\ -1 & 5 & -1 \\ 0 & -1 & 0 \end{pmatrix}$
  Оно вычисляет новое значение как 5×центр – сумма соседей, что подчёркивает перепады яркости на границе. В шейдере это реализуется выборкой текущего пикселя и 4 соседей (вверх, вниз, влево, вправо). Можно также добавить угловых соседей для более мягкого эффекта. Пример фрагмента шейдера резкости маски:

  ```glsl
  float center = tex2D(_MaskTex, i.uv).r;
  float left   = tex2D(_MaskTex, i.uv + float2(-_TexelSize.x, 0)).r;
  float right  = tex2D(_MaskTex, i.uv + float2( _TexelSize.x, 0)).r;
  float up     = tex2D(_MaskTex, i.uv + float2(0,  _TexelSize.y)).r;
  float down   = tex2D(_MaskTex, i.uv + float2(0, -_TexelSize.y)).r;
  float sharpened = center * 5.0 - (left + right + up + down);
  // Опционально: вернуть значения в 0-1 и снова бинаризовать
  sharpened = saturate(sharpened);
  float result = sharpened > 0.5 ? 1.0 : 0.0;
  return float4(result, result, result, 1);
  ```

  Этот фильтр **очень дешёв** на GPU (5 выборок на пиксель) и способен вернуть чёткость размытой билинейно границе. Однако будьте осторожны: усиление резкости может усилить и шум/дребезг маски от кадра к кадру. В сочетании с временной фильтрацией (см. далее) эффект будет более стабильным.

Общие преимущества GPU-постобработки: фильтры применяются параллельно ко всем пикселям на видеокарте, поэтому 640×480 маска обрабатывается за доли миллисекунды. Можно комбинировать несколько операций в одном шейдере (например, сделать размытие + порог + окрашивание цвета в одном проходе). Это устранит лишние копирования текстур и минимизирует задержку. В итоге, **убираем CPU из цепочки**: камера → (текстура камеры) → модель (GPU) → маска (GPU RenderTexture) → **шейдер улучшения** (GPU) → отображение.

**Использование ComputeShader**: Альтернатива материалу – написать Compute Shader, который считывает исходную маску и записывает в выходную текстуру обработанные данные. Это даёт больше контроля (например, удобнее усреднять несколько кадров, чем в пиксельном шейдере). Однако для простых фильтров разницы в производительности почти нет; шейдер-материал в пассе постобработки сделать проще. ComputeShader стоит применять, если вы собираетесь делать нетривиальную логику (например, комбинировать **N предыдущих масок** для временной стабилизации – см. ниже).

## Временная стабилизация сегментации (temporal smoothing)

Даже после пространственного сглаживания могут оставаться эффекты «дрожания» краёв между кадрами, особенно если модель даёт немного нестабильные предсказания (что типично при низком разрешении). **Временное сглаживание** уменьшает этот шум, учитывая маски предыдущих кадров. ARKit/ARFoundation применяет подобные техники для глубины и людей – при включении опции *Temporal Smoothing* маска становится значительно стабильнее от кадра к кадру. В вашем приложении можно реализовать это самостоятельно несколькими способами:

* **Экспоненциальное сглаживание (скользящее среднее)**: хранится накопленная маска, которая каждый кадр обновляется как взвешенная сумма предыдущего состояния и нового. Например, зададим коэффициент \$\alpha = 0.7\$ (сглаживание \~ трех кадров). Обновление:
  $M_{\text{new}}(x,y) = \alpha \cdot M_{\text{prev}}(x,y)\; +\; (1-\alpha) \cdot M_{\text{current}}(x,y)$
  Здесь \$M\$ – значение маски (0–1) в пикселе. При \$\alpha=0.7\$ маска в каждом пикселе обновляется на 30% текущими данными, на 70% хранит прошлое. Это сглаживает резкие изменения: случайные всплески или пропадания “стены” будут подавлены, если в прошлых кадрах там была стена. Реализация: можно делать на GPU, передавая предыдущую сглаженную текстуру как вход в шейдер наряду с текущей маской. Шейдер выполняет простое lerp-смешивание двух текстур. Для сохранения результата заведите два `RenderTexture` и чередуйте (ping-pong): один хранит сглаженную маску с прошлого кадра, в другой пишется новый результат, затем swap.

* **Усреднение N кадров**: Можно накопить несколько последних масок и усреднить их равномерно. Это близко к экспоненциальному сглаживанию, только с «прямоугольным» окном. Например, храня 5 последних кадров и вычисляя среднее, вы сильно снизите шум. Однако хранение 5 текстур 640×480 и их суммирование – небольшая память и вычислительная нагрузка. В ComputeShader можно каждый кадр обновлять скользящее окно (вычитая вклад самого старого кадра и добавляя новый). Это усложняет реализацию. Чаще достаточно проще – **экспоненциального сглаживания**, которое достигает схожего эффекта и легко реализуется.

После получения усреднённой маски возникает вопрос отображения: оставлять ли её размытой (оттенки серого) или снова делать бинарной? Если цель – **визуально показать маску на экране**, возможно, небольшая полупрозрачность по краям даже удобна для глаза (анти-алиасинг). В этом случае можно не жёстко пороговать каждый кадр, а позволить маске иметь промежуточные значения 0…1, трактуя их как альфа-прозрачность. Например, если делать overlay цветом по маске, значение 0.8 может означать 80% непрозрачности – таким образом дрожащий край будет слегка мигать прозрачностью, но не появляться/исчезать резко пиксель-в-пиксель. **ARKit** фактически выдаёт сглаженную маску глубины/стен, которая не строго бинарна, а более «мягкая», за счёт чего достигается устойчивость.

Если же требуется **чёткая бинарная маска для расчётов** (например, для окклюзии объектов), можно пороговать усреднённую маску с неким гистерезисом. Например: пиксель становится стеной, только если его значение >0.5 и *на протяжении последних N кадров* не опускалось ниже, и наоборот, убирается, если уверенность упала заметно. Это предотвращает мерцание (flicker) на границе принятия решения. Такой подход уже более сложен и может потребовать отслеживать состояние каждого пикселя (что плохо масштабируется). Поэтому обычно достаточно простого сглаживания. В любом случае, **временная стабильность** – ключ к приятному визуальному восприятию: Unity AR Occlusion Manager подчёркивает, что temporal smoothing “уменьшает шум и даёт более постоянную окклюзию”. В вашем случае это даст более постоянные границы маски от кадра к кадру.

**Пример реализации сглаживания на GPU**:

```csharp
// Инициализация
RenderTexture accumulatedMaskA, accumulatedMaskB;
accumulatedMaskA = new RenderTexture(640,480,0,RenderTextureFormat.R8);
accumulatedMaskB = new RenderTexture(640,480,0,RenderTextureFormat.R8);
accumulatedMaskA.Create(); accumulatedMaskB.Create();
Material temporalMat; // материал с шейдером, делающим blend
temporalMat.SetFloat("_Alpha", 0.7f);  // коэффициент старого кадра
// ... при каждом новом кадре после получения текущей маски (currMaskTexture):
temporalMat.SetTexture("_PrevTex", accumulatedMaskA);
temporalMat.SetTexture("_CurrTex", currMaskTexture);
Graphics.Blit(null, accumulatedMaskB, temporalMat);
// Теперь accumulatedMaskB содержит сглаженный результат.
// Swap буферов:
Swap(ref accumulatedMaskA, ref accumulatedMaskB);
// Используем accumulatedMaskA как актуальную сглаженную маску для отображения.
```

Шейдер `temporalMat` просто читает `_PrevTex` и `_CurrTex` и смешивает: `fragColor = lerp(_CurrTex, _PrevTex, _Alpha)`. Можно добавить там же и контрастное усиление, чтобы результат был опять ближе к бинарному.

**Важно:** при быстром движении камеры сглаживание может вызывать “шлейфы” – маска отстаёт от изображения. Однако для статичных объектов (стены) это обычно незаметно, так как сами стены остаются на месте относительно мира. Если же камера вращается, граница стен в маске тоже смещается; сглаживание приведёт к лёгкому запаздыванию маски, но человеческий глаз, как правило, воспринимает это как приемлемое плавное изменение, а не резкое мерцание. Можно настроить коэффициент \$\alpha\$ по вкусу (например, 0.5 даст меньше лага, но и меньше подавления шума).

Отметим, что улучшение стабильности маски во времени – общая рекомендация в видеосегментации. Хотя требует “кастомной реализации”, ваша задача как раз позволяет её встроить.

## Отображение маски на UI (RawImage) и AR Overlay: советы

**RawImage + Canvas** – распространённый способ наложить сегментационную маску поверх камеры. Чтобы итоговый результат выглядел хорошо, учитывайте следующее:

* **Совпадение размеров и аспектов**: Убедитесь, что `RawImage` заполняет экран и имеет правильное соотношение сторон. Если ваше `RenderTexture` 640×480 (соотношение 4:3) на устройстве с экраном 16:9, вы можете получить растяжение. В таких случаях либо скорректируйте размер RenderTexture под аспект камеры, либо используйте режим `Aspect Ratio Fitter`/`Preserve Aspect` на RawImage, чтобы маска не искажалась. Иначе даже сглаженная маска будет выглядеть неровно из-за масштабирования по X/Y.

* **Материал для RawImage**: Вы можете применить пользовательский **Material** к RawImage, который будет выполнять пост-обработку маски при рисовании UI. Например, материал-шейдер может считать маску (\_MainTex) и сразу окрашивать стены нужным цветом с определённой прозрачностью, выполнять пороговую функцию или фильтрацию. Это позволяет объединить несколько шагов: например, шейдер для RawImage может одновременно делать билинейную интерполяцию (через семплирование \_MainTex) *и* повышать резкость/контраст. В итоге, вместо отображения «как есть», UI-элемент сам улучшает картинку маски. Это особенно удобно, поскольку UI-рендеринг идёт на GPU и прекрасно справляется с такими задачами.

* **Alpha-прозрачность и режим наложения**: Если вы хотите, чтобы маска выглядела как выделение стен (например, полупрозрачным цветом), настройте RawImage color с нужным alpha либо, лучше, сделайте шейдер, который использует значение маски как альфу. Например, белая стена может отрисовываться с альфа = 0.5 там, где маска =1, и альфа=0 там, где 0. Тогда края маски автоматически полу-прозрачные (если маска сглажена градиентами). **Важно**: убедитесь, что Canvas/UI не делает дополнительного сглаживания пикселей маски. Если `Canvas.pixelPerfect` включён, отключите его для этого UI, чтобы разрешить суб-пиксельную интерполяцию альфы.

* **Ограничение по слою**: Если используете AR Foundation, на камере обычно висит компонент `ARCameraBackground`, который отрисовывает видео с камеры *после* обычной геометрии, но *до* UI. Поэтому размещение RawImage на Overlay Canvas поверх гарантирует, что оно покроет изображение камеры. Если нужно, чтобы виртуальные объекты *вписывались* в маску (например, скрывались за стеной), более правильно будет не через UI, а через шейдер в рендер-пассе делать маскирование. В Unity можно взять текстуру маски и использовать её как матрицу трафарета (stencil) или альфа-карту для отсечения рендеринга 3D объектов. Но это уже выходит за рамки «визуального качества» и затрагивает логику окклюзии. Тем не менее, для **чисто визуального эффекта** UI-наложение – самый простой путь.

* **Оптимизация**: Отключите у RawImage флаг **Raycast Target**, чтобы лишний раз UI-система не проверяла попадания по этому элементу (маска ведь, вероятно, не интерактивна). Это небольшая оптимизация, но её упоминают как хорошую практику для наложений. Также, если обновление маски идёт не каждый кадр (230–250 мс на кадр \~ 4 FPS), Canvas не будет перерисовываться каждый frame на 60 FPS впустую. Но если вы используете **AR Unity UI** с постоянным redraw, можно рассмотреть перевод маски в материал в самом рендере камеры (например, через Custom Render Pass в URP), чтобы он обновлялся только по готовности новых данных.

Наконец, следите за тем, **как цвет маски сочетается с фоном**. Если у вас сейчас красный или синий прямоугольник, возможно, для отладки, то для финального UX вы можете выбрать более мягкий полупрозрачный оттенок, чтобы пользователь видел и реальный мир, и выделение стены. Края, улучшенные методами выше, будут выглядеть аккуратно и естественно.

## Сводная таблица методов улучшения и их затрат

| Метод улучшения                                | Эффект на маску (качество)                                                                                                                                    | Затраты (производительность / сложность)                                                                                                                                                                                                                   |
| ---------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Билинейный апскейлинг**                      | Снижает пикселизацию за счёт интерполяции соседних пикселей. Края становятся более сглаженными, но слегка размытыми.                                          | Почти бесплатно (встроенная функция GPU) – выполняется автоматом при несоответствии размеров или через Blit. Реализуется легко (настройка FilterMode или Blit).                                                                                            |
| **Бикубический/другой апскейлинг**             | Даёт ещё более гладкое увеличение, потенциально сохраняя резкость лучше, чем билинейный. Края выглядят чётче при том же увеличении.                           | Умеренные затраты – потребует собственный шейдер с большим количеством выборок (16 или более на пиксель). Сложность реализации высокая (писать формулу интерполяции вручную).                                                                              |
| **Гауссово размытие + порог**                  | Сглаживает зазубрины на границах маски, размывая их и затем вновь делая края резкими через бинаризацию. Убирает «лестницу» пикселей, сохраняя чёткий контур.  | Низкие/средние затраты – размытие 5×5 требует \~25 выборок на пиксель (можно оптимизировать сепарацией на 2×5). На 640×480 это вполне быстро на мобильном GPU. Реализация: шейдер в 2 пасса; средняя сложность.                                            |
| **Морфологическое сглаживание (dilate+erode)** | Удаляет одиночные выступы и дырочки на маске, превращая ступенчатый край в более ровный. Контуры становятся плавнее и чуть более округлыми.                   | Средние затраты – 2 прохода с 8–9 выборками каждый (для ядра 3×3). Выполняется на GPU очень быстро. Реализация требует написать шейдер или compute (несложно), но нужно хранить временную текстуру между проходами.                                        |
| **Усиление контраста**                         | Расширяет тёмные и светлые области маски, приближая значения к 0 или 1. Усиливает «чистоту» маски, убирая серые полутона.                                     | Пренебрежимо мало (алгебраическая операция на пиксель). Легко реализовать прямо в шейдере отображения (регулируем кривую яркости).                                                                                                                         |
| **Фильтр резкости**                            | Подчёркивает границы: переход между 0 и 1 становится резче, визуально делая края маски более чёткими. Может компенсировать размытие от билинейного апскейла.  | Низкие затраты – 5 выборок на пиксель (ядро 3×3), выполняется мгновенно на GPU. Реализация несложная (shader convolution). Нужно применять осторожно, т.к. усиливает шум.                                                                                  |
| **Временное усреднение**                       | Уменьшает дрожание маски между кадрами, сглаживая случайные ошибки сегментации. Границы ведут себя стабильнее, без мерцания.                                  | Низкие/средние затраты – хранение 1–2 дополнительных масок в памяти. Каждый кадр один шейдерный проход смешивания (очень дёшево). Слегка повышает задержку обновления маски. Реализация умеренной сложности (необходим контроль за буферами, шейдер lerp). |
| **Полный GPU-пайплайн (шейдер в UI)**          | Обеспечивает максимальную плавность и быстродействие: все улучшения (фильтрация, цветность) делаются на GPU в реальном времени. Исключает копирование на CPU. | Низкие затраты во время выполнения (всё на GPU). Требуется написать пользовательский шейдер для RawImage или использовать URP Custom Render Pass – средняя сложность, но единоразово.                                                                      |

Как видно, все предложенные техники относительно недороги по производительности, особенно учитывая, что сама модель занимает \~230 мс (значительно больше). Основная работа – это инженерная реализация и тестирование комбинации методов, дающей наилучший результат. Например, можно комбинировать: билинейный апскейлинг → небольшое размытие → порог → temporal smoothing. Или морфологию + temporal. Попробуйте различные сочетания на ваших данных.

**Рекомендация по внедрению:** начните с самого простого – включите билинейный фильтр и убедитесь, что Sentis выводит маску с интерполяцией (в Sentis 1.3+ должно быть по умолчанию). Затем переместите существующие CPU-фильтры в шейдер – получите выигрыш по времени. Далее добавьте временное сглаживание (даст сразу заметный эффект стабильности). И наконец, экспериментируйте с дополнительным размыванием или морфологией для идеальных границ. Каждое улучшение по отдельности даст инкрементальное качество, а вместе – маску стен, которая выглядит гладко наложенной на реальный мир, **без «лего-эффекта» пикселей** и без дергания. Это значительно повысит восприятие AR-сцены конечным пользователем.

Gemini AI:

Оптимизация Системы Сегментации Стен в AR-Приложении для Достижения Качества "Dulux Visualizer"I. РезюмеВ данном отчете представлен анализ текущих проблем и предложены стратегии оптимизации для системы сегментации стен в AR-приложении. Основные выявленные проблемы включают: значительные узкие места в производительности, приводящие к общему времени обработки кадра 250-300 мс; выраженную визуальную блочность (пикселизацию) масок сегментации; а также опасения относительно стабильности и пригодности стандартного механизма обнаружения плоскостей AR Foundation для высококачественного "окрашивания стен". Предлагаемая стратегия решения является многогранной и охватывает оптимизацию конвейера машинного обучения (ML), улучшение качества рендеринга маски и усовершенствование представления геометрии AR и взаимодействия с ней. Конечной целью является достижение плавного, визуально привлекательного и интуитивно понятного пользовательского опыта, сопоставимого с приложениями уровня Dulux Visualizer.II. Анализ Текущей Системы и Идентификация ПроблемЭтот раздел посвящен детальному разбору существующей системы на основе предоставленной документации, логов, изображений, видео и репозитория GitHub для формирования базового понимания проблем.A. Визуальные Артефакты: Блочность Маски СегментацииНаиболее заметной визуальной проблемой является сильная блочность или пикселизация маски сегментации, которая проявляется как на RawImage в пользовательском интерфейсе, так и на AR-плоскостях, что отчетливо видно на предоставленных Изображении 1, Изображении 2 и видео.Анализ первопричин:
Низкое разрешение выходного тензора модели: Основной причиной является то, что ML-модель (Segformer) генерирует тензор с низким разрешением. Логи подтверждают это: Raw outputTensor shape: (1, 4, 120, 160), dataType: Float. Это требует значительного увеличения масштаба (апскейлинга) до целевого разрешения segmentationMaskTexture (предположительно 640x480).
Качество апскейлинга: Метод, используемый для апскейлинга этого тензора 120x160 до 640x480 (что представляет собой увеличение от 3 до 4 раз), имеет решающее значение. В документе с описанием проблем упоминается необходимость исследования SentisCompat.RenderTensorToTexture.

Методы TextureConverter.ToTexture и TextureConverter.RenderToTexture библиотеки Sentis (используемые SentisCompat) применяют "линейную передискретизацию" (linear resampling), если размеры не совпадают.1 Метод TextureTransform.SetDimensions также использует линейную передискретизацию.4
"Линейная передискретизация" обычно понимается как билинейная фильтрация. Однако документация Sentis 1 не гарантирует высококачественную билинейную фильтрацию, особенно при больших коэффициентах апскейлинга, и не предлагает явного контроля над типом фильтра (например, ближайший сосед, бикубическая) во время этой операции преобразования тензора в текстуру. Это отсутствие явного контроля и гарантии качества для встроенного апскейлинга представляет собой серьезный риск для визуального качества. Поскольку выходной тензор имеет низкое разрешение, он должен быть увеличен для отображения на экране. Если основной механизм Sentis для этого (RenderTensorToTexture или ToTexture) использует базовый или неоптимизированный билинейный фильтр, это, вероятно, приведет к блочным результатам при увеличении в 3-4 раза, что и подтверждается предоставленными изображениями.


Настройки фильтрации текстур: Установка segmentationMaskTexture.filterMode = FilterMode.Bilinear; (упомянутая в документе с проблемами и поддерживаемая 6) применяется к тому, как GPU сэмплирует из уже увеличенной текстуры при ее рендеринге (например, на AR-плоскость или в UI). Хотя это важно, данная настройка не может исправить блочность, присущую самой увеличенной текстуре.

Установка FilterMode.Bilinear является необходимым, но не достаточным условием. Если исходная текстура, передаваемая в шейдер, уже блочная из-за плохого апскейлинга, билинейная фильтрация во время рендеринга лишь слегка сгладит края блоков, но не устранит саму блочность. FilterMode влияет на то, как шейдер читает из текстуры. Если сама текстура была создана путем апскейлинга очень маленького изображения с использованием метода ближайшего соседа или билинейного метода низкого качества, блочная структура уже "запечена" в текстуру. Последующая билинейная выборка не может создать детали, которых нет.


B. Узкие Места Производительности и ЗадержкиОбщее время обработки кадра составляет 250-300 мс, что далеко от реального времени (целевое значение < 33-50 мс для 20-30 FPS).Анализ логов (ключевые показатели производительности - KPIs):
ExecuteModel: 112,30ms - Наибольший единичный вклад.
RenderTensorToTexture: 79,94ms - Второй по величине, потенциально включает апскейлинг.
TextureToTensor: 38,37ms - Значительные затраты на подготовку входных данных.
XRCpuImage.ConvertAsync: 15,13ms (первый кадр) / 26,41ms (второй кадр) - Конвертация данных с камеры.
CPU PostProcess: 1,41ms - В настоящее время незначительно, но выполняется на CPU.
ACCUMULATED STAGES TIME (RunInferenceAndPostProcess): 232,01ms
TOTAL Coroutine Wall-Clock Time (RunInferenceAndPostProcess, includes yields): 254,44ms
Таблица 1: Текущее Распределение ПроизводительностиЭтап КонвейераСреднее Время (мс)Процент от Общего Накопленного ВремениПримечания из ЛоговXRCpuImage.ConvertAsync~20.77~8.95%Конвертация изображения с камеры (1920x1440 -> 640x480)TextureToTensor38.37~16.54%Конвертация Texture2D (640x480) в тензорExecuteModel112.30~48.40%Выполнение ML-модели (Segformer)RenderTensorToTexture79.94~34.45%Рендеринг тензора в текстуру (120x160 -> 640x480)CPU PostProcess1.41~0.61%Постобработка на CPU (Sharpen, Contrast)Общее Накопленное Время (сумма)252.79100%Сумма этапов в RunInferenceAndPostProcessОбщее Время Корутины (Wall-Clock)254.44-Включая ожидания (yield)Примечание: XRCpuImage.ConvertAsync усреднено по двум приведенным значениям. Сумма этапов немного отличается от лога из-за этого усреднения, но процентное соотношение остается репрезентативным.Представленная таблица наглядно демонстрирует, на каких этапах конвейера тратятся основные ресурсы времени выполнения. Очевидно, что выполнение модели (ExecuteModel) и операция рендеринга тензора в текстуру (RenderTensorToTexture) в совокупности занимают приблизительно (112.30 + 79.94) / 232.01 ≈ 83% от общего накопленного времени обработки, что делает их абсолютными приоритетами для оптимизации.Неэффективность входного конвейера:
Камера выдает кадры высокого разрешения (например, 1920x1440, согласно логам: XRCpuImage.ConvertAsync:... (Width: 1920, Height: 1440)).
XRCpuImage.ConvertAsync конвертирует их в sourceTextureForInference с разрешением 640x480.
TextureToTensor конвертирует эту текстуру 640x480 в тензор.
Модель (segformer-model.sentis) затем выводит тензор 120x160.
Модели типа Segformer, особенно с легкими основами, такими как MiT-B0, часто оперируют на входных разрешениях значительно меньших, чем 640x480 (например, 224x224, или до 512x512 для более крупных вариантов 8). Если модель внутренне уменьшает входное изображение 640x480, это является избыточной обработкой. Этап XRCpuImage.ConvertAsync в идеале должен конвертировать изображение напрямую в оптимальное входное разрешение модели.14 Это уменьшило бы размер данных для TextureToTensor и потенциально ускорило бы выполнение модели, если внутреннее уменьшение масштаба является фактором. Логи показывают, что изображение с камеры (1920x1440) конвертируется в 640x480 для инференса, а выход модели – 120x160. Эта последовательность подразумевает несколько этапов изменения размера. Если модель Segformer внутренне изменяет размер входного изображения 640x480 на что-то меньшее (например, 224x224) перед обработкой, то начальное преобразование в 640x480 является ненужным промежуточным шагом, который обрабатывает больше данных, чем требуется для TextureToTensor и для собственной предварительной обработки модели. Прямое преобразование изображения с камеры в фактическое входное разрешение модели было бы более эффективным.
Постобработка на CPU: Логи Sharpen применен (CPU). и Contrast применен (CPU). подтверждают использование CPU, несмотря на enablePostProcessing = false, поскольку отдельные флаги, такие как enableContrast = true, по-видимому, переопределяют эту общую настройку.C. Проблемы с Обнаружением Плоскостей AR и ГеометриейПользователь выразил сомнение, что стандартный механизм обнаружения плоскостей ARFoundation подходит для задачи "покраски стен" в стиле Dulux Visualizer, где требуется точное наложение цвета. Наблюдаемое поведение (видео/изображения) включает "затекание" сегментации за края объектов и неидеальное совпадение плоскостей со сложными структурами стен.Ошибка мешинга ARKit: Лог Failed to initialize subsystem ARKit-Meshing [error: 1].
Эта ошибка может возникать, если устройство не поддерживает мешинг (например, старые iPhone, такие как 6s 15). Однако используемое устройство оснащено графическим процессором Apple A15, который является высокопроизводительным и должен поддерживать мешинг ARKit.
Следовательно, сбой инициализации мешинга, скорее всего, связан с конфигурацией проекта, несовместимостью версий Unity/пакетов или специфической ошибкой программного обеспечения iOS, а не с фундаментальным аппаратным ограничением.15 Эту проблему необходимо решить, если рассматривается использование ARMeshManager для создания динамической, более точной геометрии стен. Логи ясно указывают на сбой подсистемы ARKit-Meshing. Хотя 15 предполагает, что это может означать отсутствие поддержки устройством, устройство с A15 должно ее поддерживать. Таким образом, проблема вряд ли связана с аппаратными возможностями. Другими причинами могут быть: неверные настройки проекта, связанные с ARKit, конфликты версий между ARFoundation/ARKit XR Plugin и версией Unity (2022.3.61f1) или даже ошибка в конкретной версии iOS на тестовом устройстве. Устранение этой проблемы является необходимым условием для использования ARMeshManager.
ARMeshManager может предоставлять динамические меши окружения.17 Если проблему с мешингом ARKit удастся устранить, это может предложить более детализированную геометрию, чем простые плоскости. 18 (пример для Lightship, но принципы применимы) показывает, как ARMeshManager можно использовать с префабом для создания мешей с коллайдерами.
D. Несоответствие Опыту "Dulux Visualizer"Текущее состояние приложения (согласно видео и изображениям) демонстрирует значительные задержки, блочную маску и неточное нанесение краски по сравнению с плавным, точным и отзывчивым опытом, ожидаемым от приложений типа Dulux Visualizer.19Достижение целевого пользовательского опыта — это не просто исправление одной проблемы, а требует комплексного улучшения производительности, качества маски, взаимодействия с AR и, возможно, точности рендеринга самой краски. Пользователь ставит целью качество "Dulux Visualizer". Предоставленные материалы (видео, изображения, логи) показывают, что текущее приложение далеко от этого. Dulux Visualizer (согласно 19) подразумевает производительность в реальном времени, чистую сегментацию и точный AR-рендеринг. Текущее приложение не соответствует этим критериям. Следовательно, необходим комплексный подход, а не изолированные исправления.III. Технический Анализ и РекомендацииЭтот раздел предоставляет подробные технические решения и стратегии.A. Улучшение Качества Маски Сегментации (Устранение Блочности)1. Оптимизация Апскейлинга Выхода МоделиПроблема: Выходной тензор модели segformer-model.sentis размером 120x160 подвергается значительному апскейлингу, что приводит к блочности.Использование TextureTransform из Sentis для апскейлинга 4:
TextureConverter.RenderToTexture (или ToTexture) может принимать объект TextureTransform. Этот объект имеет метод SetDimensions(width, height, channels).
Если выходные размеры, указанные в SetDimensions, больше, чем эффективные H, W входного тензора, Sentis выполняет "линейную передискретизацию".
Рекомендация: Явно использовать TextureTransform с SetDimensions при вызове SentisCompat.RenderTensorToTexture для контроля выходного размера, чтобы он соответствовал segmentationMaskTexture (640x480).
Пример кода (концептуальный):
C#// В WallSegmentation.cs, в методе RunInferenceAndPostProcess или там, где вызывается RenderTensorToTexture
var textureTransform = new TextureTransform()
   .SetDimensions(segmentationMaskTexture.width, segmentationMaskTexture.height, 4); // Предполагая 4 канала для RGBA
//...
SentisCompat.RenderTensorToTexture(outputTensor, segmentationMaskTexture, textureTransform);


Как было установлено 4, "линейная передискретизация", вероятно, является базовой билинейной фильтрацией и может быть недостаточной для увеличения в 3-4 раза. Документация не предлагает контроля над качеством или типом этой передискретизации (например, бикубической). API Sentis предоставляет TextureTransform для управления выходными размерами. Это первое, что следует попробовать для апскейлинга. Однако документация расплывчата в отношении качества "линейной передискретизации". Если этот встроенный метод по-прежнему дает неприемлемо блочные результаты даже при правильной настройке, потребуются альтернативные методы.
2. Пользовательский Шейдер Апскейлинга (Если Апскейлинг Sentis Недостаточен)Обоснование: Если внутренний апскейлинг Sentis через TextureTransform слишком блочный или слишком медленный (время RenderTensorToTexture в 79.94 мс вызывает беспокойство), пользовательский шейдер апскейлинга предлагает больше контроля и потенциал для более высокого качества.Рабочий процесс:
Рендерить низкоразрешенный outputTensor из модели в маленькую RenderTexture (например, 120x160 или нативное выходное разрешение модели, соответствующее outputTensor.shape). Это должно быть очень быстро, если Sentis не выполняет апскейлинг.3
Использовать Graphics.Blit с пользовательским шейдером для апскейлинга этой маленькой RenderTexture до целевой segmentationMaskTexture (640x480).
Варианты шейдеров:
Шейдер билинейного апскейлинга: Реализовать ручную билинейную интерполяцию в HLSL. Это обеспечивает явный контроль. 26 предоставляют контекст для Blit и ручного сэмплирования.

Аппаратная билинейная фильтрация (через FilterMode.Bilinear на исходной маленькой текстуре и предоставление Graphics.Blit возможности обработать это с помощью простого проходного шейдера) часто быстрее, но менее гибка, чем ручная 4-точечная билинейная интерполяция в шейдере. Для апскейлинга в 3-4 раза ручной контроль может быть предпочтительнее.


Шейдер бикубического апскейлинга (продвинутый): Предлагает более гладкие результаты, чем билинейный, но более затратен в вычислительном плане (требует больше выборок текстуры, например, 16)..34 Может быть избыточным для мобильных устройств, если производительность ограничена, но стоит рассмотреть, если качество является первостепенным.
Пример кода C# для Blit (концептуальный):C#// Предполагая, что lowResMaskTexture заполнена сырым выводом модели (120x160)
// и upscalingMaterial использует пользовательский шейдер апскейлинга
Graphics.Blit(lowResMaskTexture, segmentationMaskTexture, upscalingMaterial);
Пример HLSL для билинейной интерполяции (концептуальный):High-level shader language// Упрощенная логика ручной билинейной интерполяции
float2 texelSize = _SourceTex_TexelSize.xy; // Source - это текстура низкого разрешения
float2 uv_pixels = IN.uv * _SourceTex_TexelSize.zw; // UV в пикселях исходной текстуры
float2 floor_uv = floor(uv_pixels);
float2 fract_uv = uv_pixels - floor_uv;

float2 uv00 = (floor_uv + float2(0.0, 0.0)) * texelSize;
float2 uv10 = (floor_uv + float2(1.0, 0.0)) * texelSize;
float2 uv01 = (floor_uv + float2(0.0, 1.0)) * texelSize;
float2 uv11 = (floor_uv + float2(1.0, 1.0)) * texelSize;

half4 s00 = SAMPLE_TEXTURE2D(_SourceTex, sampler_SourceTex, uv00);
half4 s10 = SAMPLE_TEXTURE2D(_SourceTex, sampler_SourceTex, uv10);
half4 s01 = SAMPLE_TEXTURE2D(_SourceTex, sampler_SourceTex, uv01);
half4 s11 = SAMPLE_TEXTURE2D(_SourceTex, sampler_SourceTex, uv11);

half4 c0 = lerp(s00, s10, fract_uv.x);
half4 c1 = lerp(s01, s11, fract_uv.x);
return lerp(c0, c1, fract_uv.y);
3. Уточнение Маски После Апскейлинга (на GPU)Даже при хорошем апскейлинге края могут нуждаться в сглаживании.
Размытие по Кавасе или Гауссу 40: Применить быстрый проход шейдера размытия к увеличенной segmentationMaskTexture. Размытие по Кавасе часто эффективно для нескольких проходов. Это должно выполняться на GPU.
Билатеральный фильтр (продвинутый) 33: Размытие с сохранением краев. Более сложный и потенциально медленный, но может дать гораздо лучшие результаты, сглаживая внутри сегментированных областей без размытия резких краев. Это было бы идеально для сохранения четких границ стен.
Простые размытия быстры. Фильтры с сохранением краев, такие как билатеральный, медленнее, но лучше сохраняют детали. Выбор зависит от бюджета производительности после других оптимизаций. После апскейлинга маска все еще может иметь некоторые артефакты или алиасинг по краям. Простое размытие может смягчить их, но также смягчит важные края. Фильтр с сохранением краев, такой как билатеральный фильтр, может выборочно сглаживать, сохраняя резкость обнаруженных границ стен и одновременно сглаживая внутреннюю часть маски. Это более затратно в вычислительном плане, но напрямую способствует более качественному эффекту "покраски".
Таблица 2: Предлагаемые Методы Улучшения МаскиМетодОписаниеУстранение блочности/алиасингаОценочная Стоимость Производительности (Моб. GPU)Ожидаемое Визуальное УлучшениеСложность РеализацииАпскейлинг Sentis TextureTransformИспользование встроенной функции Sentis для изменения размера тензора до целевого разрешения текстуры.Базовое сглаживание (вероятно, билинейное). Может быть недостаточно для сильного увеличения.Средняя (зависит от эффективности реализации Sentis)УмеренноеНизкаяПользовательский шейдер билинейного апскейлингаРендеринг низкоразрешенного тензора в маленькую RT, затем Graphics.Blit с HLSL шейдером для ручной билинейной интерполяции.Лучший контроль над процессом, потенциально лучшее качество, чем у стандартного Sentis.Низкая-СредняяХорошееСредняяПользовательский шейдер бикубического апскейлингаАналогично билинейному, но используется бикубическая интерполяция (больше выборок).Более гладкие результаты, чем билинейный, особенно при большом увеличении.Средняя-ВысокаяОчень хорошееВысокаяGPU размытие по Кавасе/Гауссу (пост-апскейл)Применение быстрого размытия к уже увеличенной маске для сглаживания оставшихся артефактов.Смягчает резкие края и пикселизацию, но может размыть детали.НизкаяУмеренноеНизкая-СредняяGPU билатеральный фильтр (пост-апскейл)Применение размытия с сохранением краев к увеличенной маске. Сглаживает области, сохраняя резкость границ.Значительно уменьшает артефакты, сохраняя четкость важных границ.Средняя-ВысокаяОчень хорошееВысокаяЭта таблица позволит пользователю взвесить все за и против различных методов улучшения маски, чтобы принять обоснованное решение, исходя из своих приоритетов в отношении визуального качества, производительности и трудозатрат на разработку. Пользователю необходимо улучшить качество маски. Существует несколько способов сделать это: от использования встроенных функций Sentis до написания сложных пользовательских шейдеров. Каждый из них имеет различные последствия для производительности, визуального результата и сложности реализации. Эта таблица четко излагает эти варианты, облегчая стратегический выбор.B. Оптимизация Производительности для Частоты Кадров в Реальном Времени1. Выполнение ML-Модели (ExecuteModel - 112.30 мс)
Квантование модели 51:

Текущая модель segformer-model.sentis имеет размер 14.9 МБ. SegFormer MiT-B0 (вероятный вариант) уже относительно мал.8
Рекомендация: Квантовать веса модели до UInt8 с использованием ModelQuantizer.QuantizeWeights. Это может уменьшить размер модели и потенциально улучшить скорость вывода на некоторых бэкендах, особенно на мобильных CPU/GPU, имеющих оптимизированные пути выполнения инструкций UInt8.
Интеграция кода 51: ModelQuantizer.QuantizeWeights(QuantizationType.Uint8, ref m_model); ModelWriter.Save(FILEPATH, m_model);
Квантование иногда может привести к небольшому снижению точности. Это необходимо проверить. Логи показывают, что используется бэкенд GPUCompute. Преимущество квантования UInt8 наиболее выражено, когда оборудование имеет встроенную поддержку быстрых операций UInt8. Выполнение модели является самым большим узким местом. Квантование — это распространенный метод ускорения вывода нейронных сетей и уменьшения размера модели. Sentis предоставляет для этого инструменты. Процесс включает преобразование весов с плавающей запятой в 8-битные целые числа. Это может ускорить доступ к памяти и вычисления, если GPU хорошо поддерживает int8. Однако это сжатие с потерями, поэтому необходимо оценить влияние на качество сегментации.


Исследование более легких вариантов Segformer или прунинга (продвинутый уровень):

Хотя MiT-B0 мал, могут существовать еще более легкие версии или конфигурации, если позволяет качество.56
Прунинг модели (удаление менее важных весов или каналов) или слияние слоев — это продвинутые методы оптимизации 58, обычно выполняемые перед преобразованием в ONNX или иногда со специализированными средами выполнения. Sam Sentis имеет некоторые оптимизационные проходы 60, но глубокий прунинг/слияние обычно является задачей на уровне модели. Это долгосрочное исследование, если квантования недостаточно.


2. Тензорные Операции (RenderTensorToTexture - 79.94 мс, TextureToTensor - 38.37 мс)
Эффективный RenderTensorToTexture:

Если реализован пользовательский апскейлинг (согласно III.A.2), RenderTensorToTexture будет только копировать маленький тензор 120x160 в маленькую RenderTexture. Эта операция должна быть чрезвычайно быстрой (<< 5 мс). Текущие 79.94 мс предполагают, что он сам выполняет апскейлинг, и, возможно, неэффективно.
Рекомендация: Убедиться, что RenderTensorToTexture рендерит в RenderTexture тех же размеров, что и outputTensor, если позже используется пользовательский шейдер апскейлинга.
Интеграция кода 3: Эти документы показывают RenderToTexture(outputTensor, rt). Если rt предварительно имеет размеры, соответствующие outputTensor, апскейлинг Sentis не происходит.


Оптимизация TextureToTensor (Подготовка Входных Данных):

Прямое преобразование в разрешение входа модели: Как обсуждалось (II.B), изображение с камеры (1920x1440) преобразуется в 640x480 (sourceTextureForInference), затем в тензор. Выход модели — 120x160. Необходимо определить фактическое входное разрешение модели (например, путем анализа модели ONNX или документации Segformer для MiT-B0, часто 224x224, 384x384 или 512x512 - 8).
Рекомендация: Использовать XRCpuImage.ConvertAsync для преобразования изображения с камеры напрямую в требуемое входное разрешение модели.

Интеграция кода 14: XRCpuImage.ConvertAsync(conversionParams), где conversionParams.outputDimensions установлено на нативный входной размер модели.


Предварительно выделенный входной тензор 2:

Скрипт WallSegmentation.cs должен предварительно выделять inputTensor в Awake или Start в соответствии с входными размерами модели.
В цикле обработки использовать TextureConverter.ToTensor(sourceTexture, preAllocatedInputTensor, textureTransform) для копирования данных без новых выделений памяти. sourceTexture будет Texture2D, полученной из XRCpuImage.ConvertAsync (теперь с правильным входным разрешением модели).
Пример кода 2:
C#// В WallSegmentation.cs
Tensor<float> preAllocatedInputTensor;
// В Start() или InitializeMLModel()
// Предполагая, что modelInputWidth, modelInputHeight известны
preAllocatedInputTensor = new Tensor<float>(new TensorShape(1, 3, modelInputHeight, modelInputWidth));
//...
// В RunInferenceAndPostProcess, после получения sourceTextureForInference (теперь правильного размера)
var textureTransform = new TextureTransform()
   .SetDimensions(modelInputWidth, modelInputHeight, 3); // Убедиться, что расположение тензора соответствует модели
TextureConverter.ToTensor(sourceTextureForInference, preAllocatedInputTensor, textureTransform);
// worker.Execute(preAllocatedInputTensor);




Этапы XRCpuImage.ConvertAsync (15-26 мс) и TextureToTensor (38 мс) являются дорогостоящими. XRCpuImage.ConvertAsync может изменять размер. TextureToTensor может записывать в предварительно выделенный тензор. Если изображение с камеры преобразуется непосредственно в фактический меньший входной размер модели (например, 224x224 вместо 640x480), объем данных, который должен обработать TextureConverter.ToTensor, значительно уменьшается, сокращая время его выполнения. Предварительное выделение тензора позволяет избежать накладных расходов на выделение памяти для каждого кадра. Этот комбинированный подход оптимизирует входной конвейер.


3. Постобработка на GPUВ настоящее время Sharpen и Contrast выполняются на CPU (1.41 мс). Хотя это немного, это хороший кандидат для переноса на GPU.
Рекомендация: Реализовать их как проходы шейдера (например, с использованием Graphics.Blit с материалами), если они необходимы для конечного качества. GPUSegmentationProcessor.cs, по-видимому, настроен для этого (Initialized textures at 256x256), но не полностью используется для всех эффектов.
Интеграция кода 63: Общий контекст по вычислениям на GPU и шейдерам постобработки.
4. Логика Пропуска Кадров (CAMERA_FRAME_SKIP_COUNT)В настоящее время используется для управления нагрузкой.
Рекомендация: После значительного улучшения производительности (цель <50 мс общего времени), уменьшить или исключить пропуск кадров для повышения плавности. Это следствие других оптимизаций, а не основная.
Таблица 3: Стратегии Оптимизации Производительности и Их ВлияниеОбласть ОптимизацииКонкретное ДействиеОжидаемый Выигрыш в Производительности (мс/FPS)Примечания/Сложность РеализацииКвантование ML-моделиПреобразование весов модели в UInt8 с помощью ModelQuantizer.QuantizeWeights.10-50+ мс (зависит от модели и GPU)Средняя. Требуется тестирование для оценки влияния на точность.Прямое преобразование XRCpuImage во входной размер моделиИспользовать XRCpuImage.ConvertAsync для изменения размера до фактического входного разрешения модели.10-20+ мс (на этапе TextureToTensor и ConvertAsync)Низкая-Средняя. Требуется знание точного входного разрешения модели.Предварительно выделенный входной тензорВыделять inputTensor один раз при запуске и повторно использовать его.5-10+ мс (уменьшение накладных расходов на выделение памяти)Низкая.Оптимизация RenderTensorToTextureЕсли используется пользовательский апскейлинг, рендерить в RT того же размера, что и выходной тензор модели.50-70+ мс (если текущее время включает неэффективный апскейлинг Sentis)Низкая (если пользовательский апскейлинг уже внедряется).Постобработка на GPUПеренести эффекты Sharpen и Contrast на GPU с использованием шейдеров.~1 мс (перенос с CPU на GPU)Средняя. Требуется написание шейдеров.Уменьшение пропуска кадровУменьшить CAMERA_FRAME_SKIP_COUNT после достижения целевой производительности.Улучшение плавности (косвенно)Низкая. Зависит от успеха других оптимизаций.Это приложение слишком медленное. Несколько этапов способствуют этой медлительности. Эта таблица разбивает проблему на управляемые задачи оптимизации, предлагает конкретные технические решения для каждой, ссылается на подтверждающую документацию и предоставляет способ оценки влияния. Такой структурированный подход имеет решающее значение для систематического улучшения производительности.C. Улучшение Обнаружения Плоскостей AR и Геометрии Стен1. Стабилизация Мешинга ARKit (Предварительное Условие для ARMeshManager)
Устранение ошибки Failed to initialize subsystem ARKit-Meshing [error: 1]:

Проверка настроек проекта: Убедиться, что плагин ARKit правильно установлен и настроен для сборки iOS. Проверить наличие конфликтующих плагинов.
Совместимость версий Unity/плагинов: Проект использует Unity 2022.3.6f1. Проверить совместимость с установленными версиями ARFoundation и ARKit XR Plugin (логи показывают UnityARKit для ввода, но ARKit-Meshing дает сбой). 15 и 15 обсуждают эту ошибку в контексте версий ARFoundation и возможностей устройства.
Программное обеспечение устройства: Убедиться, что версия iOS на устройстве A15 актуальна и не является бета-версией, которая может иметь проблемы с ARKit.
Минимальный тестовый случай: Создать новый, минимальный проект Unity только с ARFoundation и ARMeshManager, чтобы проверить, работает ли мешинг на целевом устройстве. Это может помочь изолировать проблемы, специфичные для проекта, от проблем, специфичных для среды.


2. Использование ARMeshManager для Динамической Геометрии Стен (Если Мешинг ARKit Исправлен)
Интеграция кода 17: ARMeshManager может генерировать треугольные меши окружения.
Реализация:

Добавить ARMeshManager к XR Origin.
Создать префаб для фрагментов меша (18 предлагает MeshFilter, MeshRenderer (необязательно, может быть невидимым) и MeshCollider).
Шейдер ARWallPaint.shader затем должен будет применяться к этим динамически генерируемым мешам вместо/в дополнение к ARPlanes.


Преимущества: Потенциально более точное представление геометрии стен, включая неровные поверхности или сложные углы, чем стандартные ARPlanes.
Проблемы: Генерация меша может быть вычислительно интенсивной. Стабильность и плотность меша требуют настройки (свойство density в ARMeshManager 17). Эффективное объединение множества мелких фрагментов меша может быть сложным.
Это значительное архитектурное изменение по сравнению с использованием только ARPlaneManager. Его следует рассматривать, если уточненные ARPlanes (следующий пункт) окажутся недостаточными. Стандартные AR-плоскости часто слишком просты для точной "покраски" на сложных реальных стенах. Возможность мешинга ARKit может генерировать гораздо более детализированную 3D-геометрию. Если это можно включить и это будет достаточно производительно, это может обеспечить гораздо лучшую поверхность для виртуальной краски, что приведет к более реалистичному эффекту "Dulux Visualizer". Однако это добавляет сложности и собственные соображения по производительности.
3. Уточнение Использования AR Plane (Если Оставаться с ARPlanes)
Фильтрация и объединение плоскостей: ARPlaneManager из ARFoundation 65 обнаруживает различные плоскости. Текущий ARPlaneConfigurator правильно устанавливает обнаружение на Vertical.
Совмещение плоскостей с маской сегментации: Это более продвинутая концепция. Может ли обнаруженная маска сегментации использоваться для уточнения границ или положения ARPlanes? Или для выборочного рендеринга краски только на тех частях ARPlane, которые соответствуют сегментированной стене? Это потребует проецирования данных маски на геометрию плоскости.
Пользовательский префаб плоскости 66: Текущий ARPlaneConfigurator назначает пользовательский материал. Геометрия самого префаба плоскости стандартная. Можно ли использовать пользовательский префаб плоскости с более адаптируемым шейдером или геометрией?
Уточнение поведения ARPlane для точного соответствия маскам сегментации нетривиально. Это может потребовать пользовательской логики генерации плоскостей или сложной работы с шейдерами для "вырезания" областей покраски. Если полный ARMeshManager слишком сложен или непроизводителен, возможно, существующий ARPlaneManager можно сделать умнее. Маска сегментации знает, где находится стена в экранном пространстве. AR-плоскости — это 3D-геометрия. Если мы сможем сопоставить маску с плоскостями, мы потенциально сможем скорректировать вершины плоскости или, что проще, использовать маску в шейдере, чтобы "красить" только те части AR-плоскости, которые ML-модель идентифицирует как стену. Это может повысить точность без полного динамического мешинга.
4. Генерация Пользовательской Геометрии из Маски Сегментации (Наиболее Продвинутый)
Интеграция кода 67: Здесь обсуждается генерация 3D-мешей из масок сегментации, хотя 67 относится к медицине, а 68 — к Kinect, принцип актуален.
Концепция: Извлечь контуры из (высококачественной) маски сегментации. Триангулировать эти контуры, возможно, проецируя их на грубую карту глубины или упрощенную геометрию сцены, для создания пользовательского 3D-меша в реальном времени.
Проблемы: Чрезвычайно сложно реализовать надежно и производительно на мобильных устройствах. Включает алгоритмы извлечения контуров (например, Marching Squares на маске), проекцию из 2D в 3D и генерацию/обновление меша в реальном времени.
Это предлагает наибольший потенциал для идеально соответствующей "краски" сегментированным областям, но является значительным усилием в области исследований и разработок. Вероятно, выходит за рамки немедленных исправлений, если у команды нет сильной экспертизы в этой области.
D. Улучшения Шейдера Окраски Стен (ARWallPaint.shader)1. Точное Нанесение Цвета
Текущий шейдер должен принимать (теперь улучшенную) маску сегментации в качестве входных данных.
Он должен применять выбранный цвет краски только к замаскированным областям.
Сохранение деталей текстуры: Если нижележащая стена имеет текстуру (например, обои, текстурированная краска), виртуальная краска в идеале должна смешиваться с этой текстурой, а не полностью перекрывать ее, для более реалистичного эффекта. Это может включать умножение цвета краски на изображение с камеры в замаскированных областях или использование режимов смешивания.69
2. Взаимодействие с Освещением 69
Для реализма виртуальная краска должна реагировать на реальные условия освещения.
Это включает сэмплирование данных оценки освещенности ARFoundation (интенсивность окружающего света, цветовая температура, направление/интенсивность основного света, если доступно) и использование их для затенения виртуальной краски.
Простой подход может заключаться в модуляции цвета краски интенсивностью окружающего света. Более сложный подход реализовал бы базовое диффузное/зеркальное освещение на основе оцененных источников света.
Чрезмерно сложные модели освещения могут быть медленными и выглядеть искусственно. Сбалансированный подход, который улавливает общую яркость и цветовой тон сцены, часто является наилучшим. Простое наложение плоского цвета будет выглядеть нереалистично. Настоящая краска взаимодействует со светом. ARKit предоставляет оценку освещенности. Эти данные (цвет окружающего света, интенсивность, иногда направленный свет) могут быть переданы в шейдер окраски стен, чтобы виртуальная краска выглядела ярче в хорошо освещенных местах и темнее в тени, а также чтобы она соответствовала цветовой температуре сцены, делая ее более интегрированной.
3. Смешивание Краев/Сглаживание
Даже с хорошей маской, сглаживание на уровне шейдера или мягкое смешивание краев там, где краска встречается с неокрашенными областями, может улучшить визуальное качество. Это может быть легкое размытие или альфа-растушевка по краям маски.
IV. Эмуляция "Dulux Visualizer": Путь к Отточенному UXЭтот раздел фокусируется на стратегиях более высокого уровня для преодоления разрыва до приложения коммерческого качества.A. Глубокий Анализ Подхода "Dulux Visualizer" 19
Ключевые особенности: Мгновенная визуализация, технология AR, подбор цвета по окружающим предметам/мебели, обмен дизайнами, заказ пробников.
Технические основы (предположительно): Вероятно, использует надежную сегментацию в реальном времени, продвинутое обнаружение или реконструкцию AR-плоскостей/мешей и сложный рендеринг для достижения своих результатов. Технологический стек явно не детализирован, но включает AR и AI для выбора цвета и визуализации.20
Dulux вложила значительные средства в эту технологию. Точное ее воспроизведение — это серьезная задача. Целью должно быть достижение сопоставимого уровня основной функциональности: отзывчивой и точной окраски стен. Пользователь хочет эмулировать Dulux Visualizer. Понимание того, что предлагает Dulux (из описаний в магазинах приложений и статей), устанавливает ориентир. Они подчеркивают мгновенную визуализацию, AR и подбор цвета. Это подразумевает, что их сегментация быстрая и точная, а их AR-рендеринг убедителен. Хотя мы не знаем их точного технологического стека, сама функциональность направляет наши собственные приоритеты разработки.
B. Итеративная Разработка и Приоритизация
Приоритет 1: Производительность. Достичь стабильной частоты кадров в реальном времени (например, 20-30 FPS), устранив узкие места, описанные в III.B. Без этого никакие другие улучшения не имеют значения.
Приоритет 2: Качество маски. Устранить блочность с помощью методов апскейлинга и уточнения из III.A. Чистая маска необходима для хорошего внешнего вида краски.
Приоритет 3: Геометрия AR и шейдер краски. Улучшить геометрию стен (III.C) и рендеринг краски (III.D) для точности и реализма.
Приоритет 4: Расширенные функции. Рассмотреть такие функции, как подбор цвета, сохранение дизайнов и т. д., как только основной опыт будет стабильным.
C. Взаимодействие с Пользователем и Обратная Связь
Интуитивно понятные элементы управления для выбора цветов и нанесения краски.
Четкая визуальная обратная связь во время сегментации и покраски.
Обработка крайних случаев (плохое освещение, быстрое движение, нетекстурированные стены).
V. Бенчмаркинг, Профилирование и Стратегия ИтерацийA. Определение Ключевых Показателей Производительности (KPI)
Время кадра (мс): Цель < 50 мс (для 20 FPS), в идеале < 33 мс (для 30 FPS).
Время выполнения отдельных этапов конвейера: Отслеживать время для ExecuteModel, RenderTensorToTexture, TextureToTensor, XRCpuImage.ConvertAsync.
Качество маски сегментации: Субъективная оценка, но также можно использовать метрики, такие как гладкость краев или отсутствие блочности на захваченных скриншотах.
Стабильность AR-трекинга: Измерять дрожание или смещение AR-контента.
B. Инструменты и Методы Профилирования
Unity Profiler: Необходим для профилирования CPU и GPU. Обращать внимание на ARCameraManager.OnFrameReceived, корутины вывода и вызовы рендеринга.
Xcode Instruments (для iOS): Для более глубокого профилирования на нативном уровне, особенно анализа кадров GPU и производительности Metal.
Android Profiler (если применимо): Аналогичные инструменты для Android.
Пользовательское логирование: Продолжать использовать PerformanceTracker или аналогичные средства для логирования времени выполнения конкретных этапов, как это сделано в предоставленных логах.
C. Итерационный Цикл Улучшения
Реализовать одну крупную оптимизацию: Например, квантование модели или оптимизацию входного конвейера.
Профилировать и измерить KPI: Сравнить с базовым уровнем.
Оценить визуальное качество: Проверить влияние на маску и краску.
Проанализировать и принять решение о следующем шаге: На основе результатов выбрать следующую наиболее эффективную оптимизацию.
Повторить.
VI. Заключение: Достижение Видения "Dulux Visualizer"Приоритетный План Действий (Резюме)
Производительность в первую очередь: Оптимизировать входной конвейер (прямое преобразование в размер входа модели, предварительно выделенные тензоры), квантовать модель. Стремиться к значительному сокращению времени кадра с 250-300 мс.
Устранить блочность: Реализовать пользовательский шейдер апскейлинга (начать с билинейного, оценить бикубический при необходимости) для выхода модели 120x160. При необходимости добавить размытие на GPU.
Геометрия AR: Устранить неполадки с мешингом ARKit. В случае успеха и достаточной производительности оценить ARMeshManager. В противном случае сосредоточиться на уточнении использования ARPlane или расширенном сопоставлении маски с плоскостью.
Шейдер краски: Улучшить ARWallPaint.shader для лучшего смешивания цветов и взаимодействия с освещением.
Долгосрочные Соображения
Переобучение/Дообучение модели: Если текущая модель Segformer (даже оптимизированная) не может обеспечить требуемую точность/скорость, может потребоваться изучение моделей, обученных специально для мобильных устройств или на наборах данных, более репрезентативных для внутренних сцен.56
Продвинутые эффекты рендеринга: Окклюзия на основе глубины (предотвращение "просачивания" краски на объекты переднего плана), более сложные модели освещения.
Надежность и крайние случаи: Обширное тестирование в разнообразных средах.
